accum-grad: 3
adim: 384
aheads: 2
batch-bins: 1012000
batch-sort-key: input
decoder-concat-after: false
decoder-normalize-before: false
dlayers: 6
dunits: 1536
duration-predictor-chans: 256
duration-predictor-dropout-rate: 0.1
duration-predictor-kernel-size: 3
duration-predictor-layers: 2
elayers: 6
encoder-concat-after: false
encoder-normalize-before: false
epochs: 700
eunits: 1536
grad-clip: 1.0
initial-decoder-alpha: 1.0
initial-encoder-alpha: 1.0
model-module: espnet.nets.pytorch_backend.e2e_tts_fastspeech:FeedForwardTransformer
opt: noam
patience: 0
positionwise-conv-kernel-size: 3
positionwise-layer-type: conv1d
postnet-chans: 256
postnet-dropout-rate: 0.5
postnet-filts: 5
postnet-layers: 5
pretrained-model: /data/xfding/train_result/csmsc/exp/train_no_dev_pytorch_train_fastspeech.v3.single/results/model.loss.best
reduction-factor: 1
save-interval-epoch: 50
teacher-model: /data/xfding/train_result/cmlr/exp/train_no_dev_pytorch_train_pytorch_transformer.v1.single/results/model.last1.avg.best
transfer-encoder-from-teacher: false
transformer-dec-attn-dropout-rate: 0.1
transformer-dec-dropout-rate: 0.1
transformer-dec-positional-dropout-rate: 0.1
transformer-enc-attn-dropout-rate: 0.1
transformer-enc-dec-attn-dropout-rate: 0.1
transformer-enc-dropout-rate: 0.1
transformer-enc-positional-dropout-rate: 0.1
transformer-init: pytorch
transformer-lr: 1.0
transformer-warmup-steps: 4000
use-batch-norm: true
use-scaled-pos-enc: true
weight-decay: 0.0
